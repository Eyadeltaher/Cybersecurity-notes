\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multicol}
\lstset{numbers=none, basicstyle=\ttfamily}
\renewcommand{\lstlistingname}

\geometry{a4paper, left=20mm, right=20mm, top=20mm, bottom=20mm}
\title{Directory Enumeration notes}
\date{Friday, October 3, 2025}
\author{Eyad Islam El-Taher}

\begin{document}
\maketitle

\section*{Subdomain Enumeration}

\begin{itemize}

\item \textbf{\underline{Knock Subdomain Scan}}
\begin{lstlisting}[frame=single]
knockpy -d "domain.com" --recon --bruteforce
\end{lstlisting}

\item \textbf{\underline{subfinder Scan}}
\begin{lstlisting}[frame=single]
subfinder -d someweb.com -o subf.txt -v
\end{lstlisting}

\item \textbf{\underline{assetfinder Scan}}
\begin{lstlisting}[frame=single]
assetfinder -subs-only someweb.com > asset.txt
\end{lstlisting}

\item \textbf{\underline{Subdomain Finder}}
\begin{lstlisting}[frame=single]
https://subdomainfinder.c99.nl/
Save output to a file "subfinder.txt"
\end{lstlisting}

\item \textbf{Add all Enumerated/Collected subdomains from different tools in different files into one file with unique subdomains}
\begin{lstlisting}[frame=single]
 cat subf.txt subfinder.txt asset.txt | sort -u > subdomains.txt
\end{lstlisting}

\item \textbf{\underline{To check the live subdomains and checking the status code of them}}
\begin{lstlisting}[frame=single]
cat subdomains.txt | httpx -title -wc -sc -cl -ct -location -web-server
-o alive-subdomains.txt
\end{lstlisting}
\end{itemize}


\section*{Gobuster Directory Enumeration}

\begin{itemize}

Basic scan with common options
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/ -w common.txt -o output.txt
\end{lstlisting}

With extensions
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/ -w wordlist.txt -x php,txt,html
\end{lstlisting}

Specific status codes
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/ -w wordlist.txt -s 200,301,302,403
\end{lstlisting}

status-codes-blacklist
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/ -w wordlist.txt -b 404,403
\end{lstlisting}


Set threads (faster but more noisy)
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/ -w wordlist.txt -t 50
\end{lstlisting}

\item \textbf{\underline{Advanced Example - Comprehensive Scan}}
\begin{lstlisting}[frame=single]
gobuster dir -u http://target.com/
-w /seclists/Discovery/Web-Content/directory-list-2.3-medium.txt
-x php,html,txt,js,bak,old,json
-s 200,204,301,302,307,403,500
-b 400,404,403
-t 40
-r
-o gobuster-comprehensive.txt
--timeout 10s
--user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
\end{lstlisting}

\item \textbf{\underline{Gobuster DNS command}}
\begin{lstlisting}[frame=single]
gobuster dns -q -r 8.8.8.8 -d example.com -w /Discovery/DNS/
subdomains-top1million-5000.txt -t 4 --delay 1s -o results.txt
\end{lstlisting}
NOTE --> -r  : Use custom DNS server (format server.com or server.com:port)

\item \textbf{\underline{Virtual Host Discovery:}}
\begin{lstlisting}[frame=single]
gobuster vhost -u https://example.com -t 50 
-w /wordlists/Discovery/DNS/subdomains-top1million-5000.txt
\end{lstlisting}
The vhost command discovers Virtual host names on target web servers. Virtual hosting is a technique for hosting multiple domain names on a single server.\\

Exposing hostnames on a server may reveal supplementary web content belonging to the target. Vhost checks if the subdomains exist by visiting the formed URL and cross-checking the IP address.\\

To brute-force virtual hosts, use the same wordlists as for DNS brute-forcing subdomains.\\

Similar to brute forcing subdomains eg. url = example.com, vhost looks for dev.example.com or beta.example.com etc. 

\subsection*{Tips \& Best Practices}
\begin{enumerate}
	\item Start small, then scale up to larger wordlists
	\item http:// and https:// versions of a URL
	\item Scan both http:// and https:// versions of a URL
	\item Use the -e flag to follow redirects if needed.
	\item Continue to enumerate results to find as much information as possible. Run gobuster again with the results found and see what else appears. Keep digging to locate those hidden directories. 
\end{enumerate}

\section*{FFUF Directory Enumeration}
\begin{itemize}
\item \textbf{Simple Directory Discovery:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt
\end{lstlisting}

\item \textbf{With Extensions:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -e .php,.html,.txt
\end{lstlisting}

\item \textbf{With Status Code Filtering:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -mc 200,301,302,403
\end{lstlisting}

\item \textbf{\underline{Critical Filtering Options}}

\item \textbf{Filter by Response Size (Most Important):}
\begin{lstlisting}[frame=single]
Filter out common wildcard response sizes
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 1042,1245,184

Filter size ranges
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 0-100,1000-2000

Filter code
ffuf -u http://target.com/FUZZ -w wordlist.txt -fc 401,403
\end{lstlisting}

\item \textbf{\underline{Performance \& Stealth}}

\item \textbf{Thread Control:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -t 50 # Faster
ffuf -u http://target.com/FUZZ -w wordlist.txt -t 10 # Stealthier
\end{lstlisting}

\item \textbf{Request Delay:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -p 0.1 # Fixed delay
ffuf -u http://target.com/FUZZ -w wordlist.txt -p 0.1-0.5 # Random
\end{lstlisting}

\item \textbf{Rate Limiting:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -rate 10
\end{lstlisting}

\item \textbf{\underline{Headers \& Authentication}}

\item \textbf{Custom Headers:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt
-H "User-Agent: Mozilla/5.0"
-H "Authorization: Bearer token123"
-H "X-API-Key: value"
\end{lstlisting}

\item \textbf{Host Header Fuzzing:}
\begin{lstlisting}[frame=single]
ffuf -w subdomains.txt -u https://target.com/ -H "Host: FUZZ" -mc 200
\end{lstlisting}

\item \textbf{\underline{Advanced Scanning Techniques}}

\item \textbf{Recursive Scanning:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -recursion 
-recursion-depth 2
\end{lstlisting}

\item \textbf{POST Request Fuzzing:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/login -w passwords.txt -X POST
-d "username=admin&password=FUZZ" 
-H "Content-Type: application/x-www-form-urlencoded" -mc 200 -fs 0
\end{lstlisting}

\item \textbf{Parameter Fuzzing:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/page?param=FUZZ -w parameters.txt -mc 200
\end{lstlisting}

\item \textbf{Multi-wordlist Fuzzing (Clusterbomb):}
\begin{lstlisting}[frame=single]
ffuf -w usernames.txt:USER -w passwords.txt:PASS
-u http://target.com/login?user=USER&pass=PASS
-mode clusterbomb -mc 200
\end{lstlisting}

\item \textbf{\underline{Output Options}}

\item \textbf{Save to File:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w list.txt -o result.json -of json
ffuf -u http://target.com/FUZZ -w list.txt -o result.txt -of csv
\end{lstlisting}

\item \textbf{Silent Mode:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ -w wordlist.txt -s
\end{lstlisting}
\end{itemize}

\subsection*{Practical Complete Examples}

\item \textbf{Comprehensive Directory Scan:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/FUZZ
-w /seclists/Discovery/Web-Content/directory-list-2.3-medium.txt
-e .php,.html,.txt,.js,.bak,.old
-mc 200,301,302,403,500
-fs 0,1042,1245
-t 40
-p 0.1
-c
-o ffuf-complete.json
-of json
\end{lstlisting}

\item \textbf{API Endpoint Discovery:}
\begin{lstlisting}[frame=single]
ffuf -u http://target.com/api/FUZZ
-w /seclists/Discovery/Web-Content/api/common-api-endpoints.txt
-mc 200,201,204
-H "Content-Type: application/json"
-H "Authorization: Bearer token"
-fs 0
-o api-endpoints.json
\end{lstlisting}

\item \textbf{Virtual Host Discovery:}
\begin{lstlisting}[frame=single]
ffuf -w subdomains.txt
-u http://target.com
-H "Host: FUZZ.target.com"
-mc 200,301,302
-fs 0
-o vhosts.txt
\end{lstlisting}

\subsection*{Best Practices}
\begin{itemize}
\item \textbf{Always use filters}: Start with \texttt{-fs} to filter out wildcard responses
\item \textbf{Use auto-calibration}: \texttt{-ac} helps with automatic filtering
\item \textbf{Respect rate limits}: Use \texttt{-p} or \texttt{-rate} for production systems
\item \textbf{Save your results}: Always use \texttt{-o} with appropriate format
\item \textbf{Start small}: Test with small wordlists before comprehensive scans
\item \textbf{Use recursion wisely}: \texttt{-recursion-depth} prevents infinite loops
\end{itemize}

\subsection*{Common Response Size Filters}
\begin{itemize}
\item \textbf{ASP.NET}: Often \texttt{-fs 184} for wildcard redirects
\item \textbf{WordPress}: Common sizes \texttt{-fs 1042,1245}
\item \textbf{Custom apps}: Use \texttt{-ac} to auto-detect sizes to filter
\end{itemize}



\item \textbf{\underline{Dirsearch Comprehensive Scan}}
\begin{lstlisting}[frame=single]
dirsearch -u https://target.com/ -e php,html,js,txt,json,asp,aspx 
-w /usr/share/wordlists/dirb/common.txt -t 50 --recursive-depth 2 
-o dirsearch-results.txt
\end{lstlisting}



\item \textbf{\underline{Combine and Sort Results from Multiple Tools}}
\begin{lstlisting}[frame=single]
cat gobuster-.txt ffuf-.txt dirb-*.txt dirsearch-results.txt | grep 
-Eo '(http|https)://[^/"]+' | sort -u > all-directories.txt
\end{lstlisting}

\item \textbf{\underline{Validate Live Directories with Httpx}}
\begin{lstlisting}[frame=single]
cat all-directories.txt | httpx -title -status-code -content-length 
-web-server -location -follow-redirects -o live-directories.txt
\end{lstlisting}

\item \textbf{\underline{Filter Interesting Findings}}
\begin{lstlisting}[frame=single]
cat live-directories.txt | grep 
-E "(admin|login|dashboard|config|backup|api)" > interesting-paths.txt
\end{lstlisting}

\end{itemize}

\subsection*{Important Notes}
\begin{itemize}
\item Always check robots.txt for hidden directories: \texttt{https://target.com/robots.txt}
\item Look for common backup files: .bak, .old, .txt, \_backup, \_old
\item Test for directory traversal vulnerabilities during enumeration
\item Use rate limiting (\texttt{-delay} in gobuster, \texttt{-p} in ffuf) to avoid overwhelming the target
\item Always respect the target's robots.txt and terms of service
\end{itemize}


\section*{Directory Enumeration Wordlists}

\begin{itemize}

\item \textbf{\underline{Top Recommended Wordlists}}

\item \textbf{General Purpose - Most Popular}
\begin{lstlisting}[frame=single]
/usr/share/seclists/Discovery/Web-Content/common.txt
/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-medium.txt
/usr/share/wordlists/dirb/common.txt
\end{lstlisting}

\item \textbf{Comprehensive Scanning}
\begin{lstlisting}[frame=single]
/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-big.txt
/usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt
\end{lstlisting}

\item \textbf{Quick \& Fast Scans}
\begin{lstlisting}[frame=single]
/usr/share/seclists/Discovery/Web-Content/quickhits.txt
/usr/share/seclists/Discovery/Web-Content/top-1000.txt
\end{lstlisting}

\item \textbf{API \& Modern Web Apps}
\begin{lstlisting}[frame=single]
/usr/share/seclists/Discovery/Web-Content/api/
/usr/share/seclists/Discovery/Web-Content/raft-small-words.txt
/usr/share/seclists/Discovery/Web-Content/common-api-endpoints-mazen160.txt
\end{lstlisting}

\item \textbf{\underline{Technology Specific Wordlists}}
\begin{lstlisting}[frame=single]
WordPress
/usr/share/seclists/Discovery/Web-Content/CMS/wp-plugins.fuzz.txt

Apache
/usr/share/seclists/Discovery/Web-Content/apache.txt

IIS
/usr/share/seclists/Discovery/Web-Content/iis.txt
\end{lstlisting}
\end{itemize}

\subsection*{Best Practice Recommendations}
\begin{itemize}
\item \textbf{Start Small}: Use \texttt{quickhits.txt} or \texttt{common.txt} first
\item \textbf{Escalate}: Move to medium/big lists if initial scans find little
\item \textbf{Be Specific}: Use technology-specific wordlists when you know the stack
\item \textbf{Avoid Overkill}: Don't start with huge wordlists - they're slow and noisy
\end{itemize}

\section*{Reconnaissance on archive URLs}

\subsection*{What is Wayback Machine?}
Wayback machine is a digital archive of world wide web that allows users to access and view historical snapshots of web pages, enabling them to see how a website looked and functioned at various points in time. This is achieved by periodically crawling and storing copies of web pages.


\subsection*{Tools used}
\begin{itemize}
\item \textbf{waybackurls}: A golang based tool that can be used for crawling domains archived by wayback machine.
\item \textbf{unfurl}: used to perform filtration on endpoints extracted from wayback archieve using waybackurls tool
\end{itemize}

\textbf{\underline{Using waybackurls to fetch URLs from Wayback Machine:}}
\begin{lstlisting}[frame=single]
waybackurls example.com > output.txt
\end{lstlisting}

\vspace{0.5cm}
\textbf{\underline{Filtering and extracting information from URLs using unfurl:}}\\

\textbf{Enumerating subdomains}
\begin{lstlisting}[frame=single]
cat output.txt | unfurl --unique domains
\end{lstlisting}


\textbf{Searching for tokens}
\begin{lstlisting}[frame=single]
cat output.txt | unfurl --unique values | grep -E '^ey.*\..*\..*'
\end{lstlisting}


\textbf{Finding parameters}
\begin{lstlisting}[frame=single]
cat output.txt | unfurl --unique keys
cat output.txt | grep auto= | head -n 1
\end{lstlisting}


\section{Favicon Hashing for OSINT and Reconnaissance}

\subsection{Overview}
A favicon (favorite icon) is a small graphic displayed in browser tabs and bookmarks. This technique leverages the fact that organizations often reuse the same favicon across multiple assets including subdomains, internal applications, staging servers, phishing kits, and command-and-control (C2) panels. By generating MurmurHash3 fingerprints of favicons, security professionals can passively discover related infrastructure across various search engines.

\subsection{Technical Implementation}
The favicon hash can be generated using my script with the following command:
\begin{verbatim}
./icoHash.sh https://website/favicon.ico
\end{verbatim}

Or as a one-liner:
\begin{verbatim}
python3 -c "import mmh3,requests,codecs,sys; 
print(mmh3.hash(codecs.encode(requests.get(sys.argv[1]).content,'base64')))" 
"https://example.com/favicon.ico"
\end{verbatim}

\subsection{Search Engine Queries}
The generated hash can be used across multiple OSINT platforms:

\begin{itemize}
    \item \textbf{Shodan:} \texttt{http.favicon.hash:1848946384}
    \item \textbf{FOFA:} \texttt{icon\_hash="1848946384"}
    \item \textbf{Censys:} Similar hash-based filtering
    \item \textbf{ZoomEye:} Favicon hash search capabilities
\end{itemize}

\subsection{Key Use Cases}
\begin{itemize}
    \item \textbf{CDN Bypass:} Discover origin IPs behind content delivery networks
    \item \textbf{Asset Discovery:} Identify forgotten subdomains and internal portals
    \item \textbf{Threat Hunting:} Track phishing campaigns and attacker infrastructure
    \item \textbf{Attack Surface Mapping:} Locate exposed admin panels and development environments
\end{itemize}

\end{document}